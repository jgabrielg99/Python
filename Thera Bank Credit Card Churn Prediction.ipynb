{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jgabrielg99/Python/blob/main/Project_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9a54fa0f"
      },
      "source": [
        "# Credit Card Users Churn Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EaJ8AGwpM-2"
      },
      "source": [
        "## Problem Statement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3-QehJxbp0t"
      },
      "source": [
        "### Business Context\n",
        "\n",
        "The Thera bank recently saw a steep decline in the number of users of their credit card, credit cards are a good source of income for banks because of different kinds of fees charged by the banks like annual fees, balance transfer fees, and cash advance fees, late payment fees, foreign transaction fees, and others. Some fees are charged to every user irrespective of usage, while others are charged under specified circumstances.\n",
        "\n",
        "Customers’ leaving credit cards services would lead bank to loss, so the bank wants to analyze the data of customers and identify the customers who will leave their credit card services and reason for same – so that bank could improve upon those areas\n",
        "\n",
        "You as a Data scientist at Thera bank need to come up with a classification model that will help the bank improve its services so that customers do not renounce their credit cards\n",
        "\n",
        "### Data Description\n",
        "\n",
        "* CLIENTNUM: Client number. Unique identifier for the customer holding the account\n",
        "* Attrition_Flag: Internal event (customer activity) variable - if the account is closed then \"Attrited Customer\" else \"Existing Customer\"\n",
        "* Customer_Age: Age in Years\n",
        "* Gender: Gender of the account holder\n",
        "* Dependent_count: Number of dependents\n",
        "* Education_Level: Educational Qualification of the account holder - Graduate, High School, Unknown, Uneducated, College(refers to college student), Post-Graduate, Doctorate\n",
        "* Marital_Status: Marital Status of the account holder\n",
        "* Income_Category: Annual Income Category of the account holder\n",
        "* Card_Category: Type of Card\n",
        "* Months_on_book: Period of relationship with the bank (in months)\n",
        "* Total_Relationship_Count: Total no. of products held by the customer\n",
        "* Months_Inactive_12_mon: No. of months inactive in the last 12 months\n",
        "* Contacts_Count_12_mon: No. of Contacts in the last 12 months\n",
        "* Credit_Limit: Credit Limit on the Credit Card\n",
        "* Total_Revolving_Bal: Total Revolving Balance on the Credit Card\n",
        "* Avg_Open_To_Buy: Open to Buy Credit Line (Average of last 12 months)\n",
        "* Total_Amt_Chng_Q4_Q1: Change in Transaction Amount (Q4 over Q1)\n",
        "* Total_Trans_Amt: Total Transaction Amount (Last 12 months)\n",
        "* Total_Trans_Ct: Total Transaction Count (Last 12 months)\n",
        "* Total_Ct_Chng_Q4_Q1: Change in Transaction Count (Q4 over Q1)\n",
        "* Avg_Utilization_Ratio: Average Card Utilization Ratio\n",
        "\n",
        "#### What Is a Revolving Balance?\n",
        "\n",
        "- If we don't pay the balance of the revolving credit account in full every month, the unpaid portion carries over to the next month. That's called a revolving balance\n",
        "\n",
        "\n",
        "##### What is the Average Open to buy?\n",
        "\n",
        "- 'Open to Buy' means the amount left on your credit card to use. Now, this column represents the average of this value for the last 12 months.\n",
        "\n",
        "##### What is the Average utilization Ratio?\n",
        "\n",
        "- The Avg_Utilization_Ratio represents how much of the available credit the customer spent. This is useful for calculating credit scores.\n",
        "\n",
        "\n",
        "##### Relation b/w Avg_Open_To_Buy, Credit_Limit and Avg_Utilization_Ratio:\n",
        "\n",
        "- ( Avg_Open_To_Buy / Credit_Limit ) + Avg_Utilization_Ratio = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbHOIdlwcrqR"
      },
      "source": [
        "### **Please read the instructions carefully before starting the project.**\n",
        "This is a commented Jupyter IPython Notebook file in which all the instructions and tasks to be performed are mentioned.\n",
        "* Blanks '_______' are provided in the notebook that\n",
        "needs to be filled with an appropriate code to get the correct result. With every '_______' blank, there is a comment that briefly describes what needs to be filled in the blank space.\n",
        "* Identify the task to be performed correctly, and only then proceed to write the required code.\n",
        "* Fill the code wherever asked by the commented lines like \"# write your code here\" or \"# complete the code\". Running incomplete code may throw error.\n",
        "* Please run the codes in a sequential manner from the beginning to avoid any unnecessary errors.\n",
        "* Add the results/observations (wherever mentioned) derived from the analysis in the presentation and submit the same.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_-uuGqH-qTt"
      },
      "source": [
        "## Importing necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "83D17_Wl4jal",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d920a9b-a1fa-4c37-8258-592be1955d01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.0/226.0 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.1/297.1 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33m  WARNING: The scripts f2py, f2py3 and f2py3.10 are installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# Installing the libraries with the specified version.\n",
        "# uncomment and run the following line if Google Colab is being used\n",
        "!pip install scikit-learn==1.2.2 seaborn==0.13.1 matplotlib==3.7.1 numpy==1.25.2 pandas==1.5.3 imbalanced-learn==0.10.1 xgboost==2.0.3 -q --user"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Installing the libraries with the specified version.\n",
        "# uncomment and run the following lines if Jupyter Notebook is being used\n",
        "# !pip install scikit-learn==1.2.2 seaborn==0.13.1 matplotlib==3.7.1 numpy==1.25.2 pandas==1.5.3 imblearn==0.12.0 xgboost==2.0.3 -q --user\n",
        "# !pip install --upgrade -q threadpoolctl"
      ],
      "metadata": {
        "id": "XVB5DgmofkGQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reading, maniputating, and visualizing data\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# impute missing data\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# model scoring\n",
        "from sklearn import metrics\n",
        "\n",
        "# feature engineering\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, confusion_matrix\n",
        "\n",
        "# hyperparameter tuning\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "\n",
        "# balancing techniques\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "# model building\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, BaggingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "P7DHMz-xTQJS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note**: *After running the above cell, kindly restart the notebook kernel and run all cells sequentially from the start again*."
      ],
      "metadata": {
        "id": "G3tBAjBgrLAQ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8I-FQB2qrNmM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxhpZv9y-qTw"
      },
      "source": [
        "## Loading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oJnKoHy14jam",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "outputId": "933ffd69-1192-4f0f-fb44-189a6c568997"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-d5df0069828e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    131\u001b[0m   )\n\u001b[1;32m    132\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     if (\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('/content/drive/MyDrive/AIML Course/Files/BankChurners.csv')\n",
        "df = data.copy()"
      ],
      "metadata": {
        "id": "n2uv2lpOTB8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "d3-llanNT1Z6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UvpMDcaaMKtI"
      },
      "source": [
        "## Data Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tIiCRwqZ54_C"
      },
      "source": [
        "- Observations\n",
        "- Sanity checks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "01hJQ7EfMKtK"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* There are 10127 rows in this dataset and 21 features"
      ],
      "metadata": {
        "id": "3LRRliEoU3Ao"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "_e8J7_DIUCLv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Attrition_Flag, Gender, Education_Level, Marital_Status, Income_Category, Card_Category are all object data types\n",
        "* There appear to be missing values in Education_Level and Marital_Status"
      ],
      "metadata": {
        "id": "9IHMUsdJURKT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe().round(3)"
      ],
      "metadata": {
        "id": "jr4Du-bHUOnQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* From an inital look, there don't appear to be any unexpected values; however, there are certainly outliers in categories such as Credit_Limit, Avg_Open_To_Buy, Total_Amt_Chng_Q4_Q1, Total_Trans_Amt, etc. as the max value is substantially higher than even the 3rd quartile"
      ],
      "metadata": {
        "id": "KhAi-GvXWnqF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.isna().sum()"
      ],
      "metadata": {
        "id": "drD7lSgwU_zJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.loc[df['Education_Level'].isna()]"
      ],
      "metadata": {
        "id": "2hj2KWLLXy1m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.loc[df['Marital_Status'].isna()]"
      ],
      "metadata": {
        "id": "uenUtdzqYWzI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* There doesn't appear to be any pattern linking missing Education_Level or Marital_Status"
      ],
      "metadata": {
        "id": "PXUujYyJZ8m1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.duplicated().sum()"
      ],
      "metadata": {
        "id": "LjEF4ug7XUJE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['CLIENTNUM'].nunique()"
      ],
      "metadata": {
        "id": "nOY0BT2s9uzZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop(['CLIENTNUM'], axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "t59g4Ms99ywC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Attrition_Flag'] = df['Attrition_Flag'].replace({\n",
        "    'Existing Customer': 0,\n",
        "    'Attrited Customer': 1\n",
        "})"
      ],
      "metadata": {
        "id": "yTZHTnoP92zV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-yGG8LNSSMa"
      },
      "source": [
        "## Exploratory Data Analysis (EDA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bGVKmh75ri8"
      },
      "source": [
        "- EDA is an important part of any project involving data.\n",
        "- It is important to investigate and understand the data better before building a model with it.\n",
        "- A few questions have been mentioned below which will help you approach the analysis in the right manner and generate insights from the data.\n",
        "- A thorough analysis of the data, in addition to the questions mentioned below, should be done."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEyqzdJBb0jU"
      },
      "source": [
        "**Questions**:\n",
        "\n",
        "1. How is the total transaction amount distributed?\n",
        "2. What is the distribution of the level of education of customers?\n",
        "3. What is the distribution of the level of income of customers?\n",
        "4. How does the change in transaction amount between Q4 and Q1 (`total_ct_change_Q4_Q1`) vary by the customer's account status (`Attrition_Flag`)?\n",
        "5. How does the number of months a customer was inactive in the last 12 months (`Months_Inactive_12_mon`) vary by the customer's account status (`Attrition_Flag`)?\n",
        "6. What are the attributes that have a strong correlation with each other?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YyWJgFlKlWM"
      },
      "source": [
        "#### The below functions need to be defined to carry out the Exploratory Data Analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QIP4bI3Zbp07"
      },
      "outputs": [],
      "source": [
        "# function to plot a boxplot and a histogram along the same scale.\n",
        "\n",
        "\n",
        "def histogram_boxplot(data, feature, figsize=(12, 7), kde=False, bins=None):\n",
        "    \"\"\"\n",
        "    Boxplot and histogram combined\n",
        "\n",
        "    data: dataframe\n",
        "    feature: dataframe column\n",
        "    figsize: size of figure (default (12,7))\n",
        "    kde: whether to the show density curve (default False)\n",
        "    bins: number of bins for histogram (default None)\n",
        "    \"\"\"\n",
        "    f2, (ax_box2, ax_hist2) = plt.subplots(\n",
        "        nrows=2,  # Number of rows of the subplot grid= 2\n",
        "        sharex=True,  # x-axis will be shared among all subplots\n",
        "        gridspec_kw={\"height_ratios\": (0.25, 0.75)},\n",
        "        figsize=figsize,\n",
        "    )  # creating the 2 subplots\n",
        "    sns.boxplot(\n",
        "        data=data, x=feature, ax=ax_box2, showmeans=True, color=\"violet\"\n",
        "    )  # boxplot will be created and a triangle will indicate the mean value of the column\n",
        "    sns.histplot(\n",
        "        data=data, x=feature, kde=kde, ax=ax_hist2, bins=bins, palette=\"winter\"\n",
        "    ) if bins else sns.histplot(\n",
        "        data=data, x=feature, kde=kde, ax=ax_hist2\n",
        "    )  # For histogram\n",
        "    ax_hist2.axvline(\n",
        "        data[feature].mean(), color=\"green\", linestyle=\"--\"\n",
        "    )  # Add mean to the histogram\n",
        "    ax_hist2.axvline(\n",
        "        data[feature].median(), color=\"black\", linestyle=\"-\"\n",
        "    )  # Add median to the histogram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5021de33"
      },
      "outputs": [],
      "source": [
        "# function to create labeled barplots\n",
        "\n",
        "\n",
        "def labeled_barplot(data, feature, perc=False, n=None):\n",
        "    \"\"\"\n",
        "    Barplot with percentage at the top\n",
        "\n",
        "    data: dataframe\n",
        "    feature: dataframe column\n",
        "    perc: whether to display percentages instead of count (default is False)\n",
        "    n: displays the top n category levels (default is None, i.e., display all levels)\n",
        "    \"\"\"\n",
        "\n",
        "    total = len(data[feature])  # length of the column\n",
        "    count = data[feature].nunique()\n",
        "    if n is None:\n",
        "        plt.figure(figsize=(count + 1, 5))\n",
        "    else:\n",
        "        plt.figure(figsize=(n + 1, 5))\n",
        "\n",
        "    plt.xticks(rotation=90, fontsize=15)\n",
        "    ax = sns.countplot(\n",
        "        data=data,\n",
        "        x=feature,\n",
        "        palette=\"Paired\",\n",
        "        order=data[feature].value_counts().index[:n].sort_values(),\n",
        "    )\n",
        "\n",
        "    for p in ax.patches:\n",
        "        if perc == True:\n",
        "            label = \"{:.1f}%\".format(\n",
        "                100 * p.get_height() / total\n",
        "            )  # percentage of each class of the category\n",
        "        else:\n",
        "            label = p.get_height()  # count of each level of the category\n",
        "\n",
        "        x = p.get_x() + p.get_width() / 2  # width of the plot\n",
        "        y = p.get_height()  # height of the plot\n",
        "\n",
        "        ax.annotate(\n",
        "            label,\n",
        "            (x, y),\n",
        "            ha=\"center\",\n",
        "            va=\"center\",\n",
        "            size=12,\n",
        "            xytext=(0, 5),\n",
        "            textcoords=\"offset points\",\n",
        "        )  # annotate the percentage\n",
        "\n",
        "    plt.show()  # show the plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c08fe5b8"
      },
      "outputs": [],
      "source": [
        "# function to plot stacked bar chart\n",
        "\n",
        "def stacked_barplot(data, predictor, target):\n",
        "    \"\"\"\n",
        "    Print the category counts and plot a stacked bar chart\n",
        "\n",
        "    data: dataframe\n",
        "    predictor: independent variable\n",
        "    target: target variable\n",
        "    \"\"\"\n",
        "    count = data[predictor].nunique()\n",
        "    sorter = data[target].value_counts().index[-1]\n",
        "    tab1 = pd.crosstab(data[predictor], data[target], margins=True).sort_values(\n",
        "        by=sorter, ascending=False\n",
        "    )\n",
        "    print(tab1)\n",
        "    print(\"-\" * 120)\n",
        "    tab = pd.crosstab(data[predictor], data[target], normalize=\"index\").sort_values(\n",
        "        by=sorter, ascending=False\n",
        "    )\n",
        "    tab.plot(kind=\"bar\", stacked=True, figsize=(count + 1, 5))\n",
        "    plt.legend(\n",
        "        loc=\"lower left\", frameon=False,\n",
        "    )\n",
        "    plt.legend(loc=\"upper left\", bbox_to_anchor=(1, 1))\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e90985c5"
      },
      "outputs": [],
      "source": [
        "### Function to plot distributions\n",
        "\n",
        "def distribution_plot_wrt_target(data, predictor, target):\n",
        "\n",
        "    fig, axs = plt.subplots(2, 2, figsize=(12, 10))\n",
        "\n",
        "    target_uniq = data[target].unique()\n",
        "\n",
        "    axs[0, 0].set_title(\"Distribution of target for target=\" + str(target_uniq[0]))\n",
        "    sns.histplot(\n",
        "        data=data[data[target] == target_uniq[0]],\n",
        "        x=predictor,\n",
        "        kde=True,\n",
        "        ax=axs[0, 0],\n",
        "        color=\"teal\",\n",
        "    )\n",
        "\n",
        "    axs[0, 1].set_title(\"Distribution of target for target=\" + str(target_uniq[1]))\n",
        "    sns.histplot(\n",
        "        data=data[data[target] == target_uniq[1]],\n",
        "        x=predictor,\n",
        "        kde=True,\n",
        "        ax=axs[0, 1],\n",
        "        color=\"orange\",\n",
        "    )\n",
        "\n",
        "    axs[1, 0].set_title(\"Boxplot w.r.t target\")\n",
        "    sns.boxplot(data=data, x=target, y=predictor, ax=axs[1, 0], palette=\"gist_rainbow\")\n",
        "\n",
        "    axs[1, 1].set_title(\"Boxplot (without outliers) w.r.t target\")\n",
        "    sns.boxplot(\n",
        "        data=data,\n",
        "        x=target,\n",
        "        y=predictor,\n",
        "        ax=axs[1, 1],\n",
        "        showfliers=False,\n",
        "        palette=\"gist_rainbow\",\n",
        "    )\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Univariate Analysis"
      ],
      "metadata": {
        "id": "_jRX4UBK8ULO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labeled_barplot(df, 'Attrition_Flag')"
      ],
      "metadata": {
        "id": "J0tpuETBb_3r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Attrition_Flag'].value_counts(normalize=True)"
      ],
      "metadata": {
        "id": "dvglJ33ycH_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 1627 (approximately 16%) customer leave their credit card services"
      ],
      "metadata": {
        "id": "MMJTY1UzbOex"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "XckiXiGEbNuy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "histogram_boxplot(df, 'Customer_Age')"
      ],
      "metadata": {
        "id": "yIkXOpVibl9d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Normal distribution of ages with a few outliers at the older end\n",
        "* The average age is around 45-46 years old"
      ],
      "metadata": {
        "id": "bX_NlNj9bx6A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labeled_barplot(df, 'Gender')"
      ],
      "metadata": {
        "id": "vCDNXrpDbvEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Gender'].value_counts(normalize=True)"
      ],
      "metadata": {
        "id": "BApESdSkcOha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 53% of customers are female\n",
        "* 47% of customers are male"
      ],
      "metadata": {
        "id": "WYlGO4OVcTv8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "histogram_boxplot(df, 'Dependent_count')"
      ],
      "metadata": {
        "id": "_b3VK7TZb9rY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Normal distribution of the number of dependents\n",
        "* The average number of dependents is slightly above 2"
      ],
      "metadata": {
        "id": "kEF6OJZkc0qo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labeled_barplot(df, 'Education_Level')"
      ],
      "metadata": {
        "id": "4x01BBMGcoDJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Education_Level'].value_counts(normalize=True)"
      ],
      "metadata": {
        "id": "NP7aiYKldSEJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The majority of customers (36%) hold a Graduate Degree\n",
        "* The fewest number of customers (5%) hold a Doctorate"
      ],
      "metadata": {
        "id": "NgOV8__vdOSg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labeled_barplot(df, 'Marital_Status')"
      ],
      "metadata": {
        "id": "yE_YY0prdMbV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Marital_Status'].value_counts(normalize=True)"
      ],
      "metadata": {
        "id": "EquMfN_HdxLk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labeled_barplot(df, 'Income_Category')"
      ],
      "metadata": {
        "id": "0IDadAvtd2gP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* There is an unexpected value called \"abc\" in Income Category\n",
        "** We may be able to inpute likely values if there is high correlation between Income Category and another feature such as Credit Limit"
      ],
      "metadata": {
        "id": "tRMDs0T7y7u8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "Mb9n6hfDxQxH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labeled_barplot(df, 'Card_Category')"
      ],
      "metadata": {
        "id": "VQx3y8n-eBub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The Blue Card is the most commonly held credit card. Platinum is the rarest card"
      ],
      "metadata": {
        "id": "6Jg798-ZzR7X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "histogram_boxplot(df, 'Months_on_book')"
      ],
      "metadata": {
        "id": "0CJqTqNDx2Ct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Values around 35 months appear significantly more frequently than any other value\n",
        "* There are outliers on either ends of the data set"
      ],
      "metadata": {
        "id": "Iu34bhA6zlQ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "histogram_boxplot(df, 'Total_Relationship_Count')"
      ],
      "metadata": {
        "id": "yj8k1tmTzeC6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* There is a somewhat even distribution. The average number of products held is slightly below 4"
      ],
      "metadata": {
        "id": "toO-Hmvq0BwE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "histogram_boxplot(df, 'Months_Inactive_12_mon')"
      ],
      "metadata": {
        "id": "Y-R3fgZY0I2D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Most customers are inactive between 2 and 3 months in a year\n",
        "* An Inactive status of 1 month, 5 months, or 6 months are consider outliers"
      ],
      "metadata": {
        "id": "aiF6jevz0OON"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "histogram_boxplot(df, 'Contacts_Count_12_mon')"
      ],
      "metadata": {
        "id": "923cobOB0L0y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Most customers were contacted between 2 and 3 times in a year\n",
        "* There are outliers on either end of this dataset - 0 contacts, 5 contacts, and 6 contacts"
      ],
      "metadata": {
        "id": "0QS__yqF0jL7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "histogram_boxplot(df, 'Credit_Limit')"
      ],
      "metadata": {
        "id": "WYnpkXrO0ck2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Credit limit is heavily right skewed; however there is are a large number of customers at around 34k.\n",
        "** This may be the maximum possible credit limit\n",
        "* There are a large number of outliers on the higher end of the dataset"
      ],
      "metadata": {
        "id": "6Z3s_TQ502s5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "histogram_boxplot(df, 'Total_Revolving_Bal')"
      ],
      "metadata": {
        "id": "V7IcmYKT00Xu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* There are a large number of customer with no revolving balance. Otherwise, the data has somewhat normal distribution.\n",
        "* There are also a larger number of customers with a revolving balance around 2500."
      ],
      "metadata": {
        "id": "9RoJS_UM1Zxq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "L7sfnUaF1TbO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "histogram_boxplot(df, 'Avg_Open_To_Buy')"
      ],
      "metadata": {
        "id": "fS55RBut1soa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* This data is heavily right skewed. Similarly to Credit Limit, there is a moderate concentration of values between 30,000-35,000\n",
        "* There are large number of outliers on the higher end of the data"
      ],
      "metadata": {
        "id": "o9hLctgI2mBA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "histogram_boxplot(df, 'Total_Amt_Chng_Q4_Q1')"
      ],
      "metadata": {
        "id": "w4QXmmq21zcZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* This data has normal distribution\n",
        "** The average falls around .75\n",
        "* There are outliers on either end of this dataset"
      ],
      "metadata": {
        "id": "C2KGUsF73DJH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "histogram_boxplot(df, 'Total_Trans_Amt')"
      ],
      "metadata": {
        "id": "K8D1DkFT3Bly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* This data is multimodal featuring 4 different peaks\n",
        "* There are a large number of outliers on the higher end of the dataset"
      ],
      "metadata": {
        "id": "iL0GKvH03vDe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "histogram_boxplot(df, 'Total_Trans_Ct')"
      ],
      "metadata": {
        "id": "GGWML5Px3uB9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Bimodal dataset with a peak around 40 and 75 transactions\n",
        "** There are few outliers at the top end of the dataset"
      ],
      "metadata": {
        "id": "0P0EbeRA4O26"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "histogram_boxplot(df, 'Total_Ct_Chng_Q4_Q1')"
      ],
      "metadata": {
        "id": "f-kruWZo4ME_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Normal distribution\n",
        "** Outliers on either end of the data"
      ],
      "metadata": {
        "id": "geDDgT0e4g4X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "histogram_boxplot(df, 'Avg_Utilization_Ratio')"
      ],
      "metadata": {
        "id": "ReLqJAkk4fW2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Right skewed\n",
        "* Related to Credit Limit and Open to Buy"
      ],
      "metadata": {
        "id": "14GTW1Rq4rcy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Bivariate Analysis"
      ],
      "metadata": {
        "id": "aY99sQCi8fmR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "ar3Xe8MH8v96"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15, 7))\n",
        "sns.heatmap(df.corr(), annot=True, vmin=-1, vmax=1, fmt=\".2f\", cmap=\"Spectral\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pLiRWPg48uhL",
        "outputId": "27d960e5-2417-499d-bf4b-1150a826f2da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'plt' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-d207b7d46160>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheatmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfmt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\".2f\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Spectral\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Total Transaction Count has the strongest correlation with Attrition_Flag\n",
        "* Total Transaction Count and Total Transation Amount have very high correlation\n",
        "** This is expected as someone who uses there card more frequently will likely have higher transaction amount\n",
        "* Months on Book and Customer Age also have a very high correlation"
      ],
      "metadata": {
        "id": "7UeAh9hIzdPA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "distribution_plot_wrt_target(df, 'Customer_Age', 'Attrition_Flag')"
      ],
      "metadata": {
        "id": "_fGAzpR9xFVK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stacked_barplot(df, 'Gender', 'Attrition_Flag')"
      ],
      "metadata": {
        "id": "-MVa5reIxKwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* A slightly higher percentage of males attrited than females"
      ],
      "metadata": {
        "id": "J_ccf1l-0psT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stacked_barplot(df, 'Dependent_count', 'Attrition_Flag')"
      ],
      "metadata": {
        "id": "Hanoep03xRbs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stacked_barplot(df, 'Education_Level', 'Attrition_Flag')"
      ],
      "metadata": {
        "id": "E5SAO4kqxU5Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* A slightly higher percentage of customers with a doctorate attrited than other education levels"
      ],
      "metadata": {
        "id": "pDFeUCN806AC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stacked_barplot(df, 'Marital_Status', 'Attrition_Flag')"
      ],
      "metadata": {
        "id": "pzYnUHf4xeUw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stacked_barplot(df, 'Income_Category', 'Attrition_Flag')"
      ],
      "metadata": {
        "id": "jTSXu9KsxhC7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stacked_barplot(df, 'Card_Category', 'Attrition_Flag')"
      ],
      "metadata": {
        "id": "1RUT-Bmcxj3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* A higher percentage of platinum card holders attrited compared to other card categories"
      ],
      "metadata": {
        "id": "0k_ZYmxY1CAf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "distribution_plot_wrt_target(df, 'Months_on_book', 'Attrition_Flag')"
      ],
      "metadata": {
        "id": "w-z_9AJixlBx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "g-CpAScbxzs_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stacked_barplot(df, 'Total_Relationship_Count', 'Attrition_Flag')"
      ],
      "metadata": {
        "id": "EiGEaEyyx4oI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* More customers with 1 or 2 products attrited than customers with more than 2 products"
      ],
      "metadata": {
        "id": "lGBYL0Lk1MBD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stacked_barplot(df, 'Months_Inactive_12_mon', 'Attrition_Flag')"
      ],
      "metadata": {
        "id": "yF91Tlnkx7Si"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Surprisingly, customer with 0 months inactive are most likely to attrite; follwed by customer with 4 months inactive\n",
        "** However, 0 months inactive is the smallest category with only 29 customers out of 10127"
      ],
      "metadata": {
        "id": "a8qodE7B1a-g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stacked_barplot(df, 'Contacts_Count_12_mon', 'Attrition_Flag')"
      ],
      "metadata": {
        "id": "fK1Oo_WXyaOw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The more times a customer contacts the bank regarding credit services, the more likely they are to attrite\n",
        "  * All customers that contacted 6 times attrited\n",
        "\n"
      ],
      "metadata": {
        "id": "xPfWDQN82I-s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "distribution_plot_wrt_target(df, 'Credit_Limit', 'Attrition_Flag')"
      ],
      "metadata": {
        "id": "wZY-fZ4nyh27"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "distribution_plot_wrt_target(df, 'Total_Revolving_Bal', 'Attrition_Flag')"
      ],
      "metadata": {
        "id": "gXZtCxGeygcC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Customers with little to no revolving balance attrited more than customers with high revolving balances\n",
        "  * This aligns with longer periods of inactivity resulting in attriting"
      ],
      "metadata": {
        "id": "e-jqrvDU3JwP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "distribution_plot_wrt_target(df, 'Avg_Open_To_Buy', 'Attrition_Flag')"
      ],
      "metadata": {
        "id": "MWxmqFDqyvPq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "distribution_plot_wrt_target(df, 'Total_Amt_Chng_Q4_Q1', 'Attrition_Flag')"
      ],
      "metadata": {
        "id": "oWfS3p1MyzRl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "distribution_plot_wrt_target(df, 'Total_Trans_Amt', 'Attrition_Flag')"
      ],
      "metadata": {
        "id": "MpR84BVly4xl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Customers that spent less on their cards attrited more often than customers who had a higher transaction amount"
      ],
      "metadata": {
        "id": "5PE6iLIw3qrU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "distribution_plot_wrt_target(df, 'Total_Trans_Ct', 'Attrition_Flag')"
      ],
      "metadata": {
        "id": "FoS-ZULGy64I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Similarly to Transaction Amounts, customers with fewer transactions are more likely to attrite"
      ],
      "metadata": {
        "id": "yeBYmud3319N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "distribution_plot_wrt_target(df, 'Total_Ct_Chng_Q4_Q1', 'Attrition_Flag')"
      ],
      "metadata": {
        "id": "ZTFZZpkWzDMb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "distribution_plot_wrt_target(df, 'Avg_Utilization_Ratio', 'Attrition_Flag')"
      ],
      "metadata": {
        "id": "FZCgDH0BzEcg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Customers that attrited generally had a lower average utilization ratio"
      ],
      "metadata": {
        "id": "SrMccg-o5eqv"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "knk0w9XH4jao"
      },
      "source": [
        "## Data Pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2JbJc1bX4jao"
      },
      "outputs": [],
      "source": [
        "# outlier detection using boxplot\n",
        "numeric_columns = df.select_dtypes(include=np.number).columns.tolist()\n",
        "\n",
        "plt.figure(figsize=(15, 12))\n",
        "\n",
        "for i, variable in enumerate(numeric_columns):\n",
        "    plt.subplot(4, 4, i + 1)\n",
        "    plt.boxplot(df[variable], whis=1.5)\n",
        "    plt.tight_layout()\n",
        "    plt.title(variable)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0J99-7Kubp09"
      },
      "source": [
        "## Missing value imputation\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* There are missing values in Income_Category, Education_Level, and Marital_Status"
      ],
      "metadata": {
        "id": "3a_R2csj8mzl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.loc[df['Income_Category'] == 'abc', 'Income_Category'] = np.nan"
      ],
      "metadata": {
        "id": "AkP37RVo8yj2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SI = SimpleImputer(strategy = 'most_frequent')"
      ],
      "metadata": {
        "id": "u1IfC2UA8_YM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isna().sum()"
      ],
      "metadata": {
        "id": "JPDym9tl9UaG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = df.drop('Attrition_Flag', axis=1)\n",
        "y = df['Attrition_Flag']"
      ],
      "metadata": {
        "id": "RkFhqYLf-FjG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split data for temp and test sets\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=1, stratify=y\n",
        ")\n",
        "\n",
        "# split temp into train and valid sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_temp, y_temp, test_size=0.25, random_state=1, stratify=y_temp\n",
        ")\n",
        "\n",
        "print(X_train.shape, X_val.shape, X_test.shape)"
      ],
      "metadata": {
        "id": "zRl46UIX-SgW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat_cols = list(X_train.select_dtypes(include='object').columns)"
      ],
      "metadata": {
        "id": "i3nW_JE0CClj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train[cat_cols] = SI.fit_transform(X_train[cat_cols])\n",
        "X_val[cat_cols] = SI.transform(X_val[cat_cols])\n",
        "X_test[cat_cols] = SI.transform(X_test[cat_cols])"
      ],
      "metadata": {
        "id": "fyW-IR5U_M8q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check for missing values in train, val, and test set\n",
        "print(X_train.isna().sum())\n",
        "print(\"-\" * 30)\n",
        "print(X_val.isna().sum())\n",
        "print(\"-\" * 30)\n",
        "print(X_test.isna().sum())"
      ],
      "metadata": {
        "id": "wvvvAvU8AVuu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = pd.get_dummies(X_train, drop_first=True)\n",
        "X_val = pd.get_dummies(X_val, drop_first=True)\n",
        "X_test = pd.get_dummies(X_test, drop_first=True)\n",
        "print(X_train.shape, X_val.shape, X_test.shape)"
      ],
      "metadata": {
        "id": "lnItnyIfHhhP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* After encoding the data, there are 29 variables in the dataset"
      ],
      "metadata": {
        "id": "ilnoIvOyII_c"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OzOa9FGA6WtG"
      },
      "source": [
        "## Model Building"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZqmoqz7bp0-"
      },
      "source": [
        "### Model evaluation criterion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2ORUgmUjDZC"
      },
      "source": [
        "The nature of predictions made by the classification model will translate as follows:\n",
        "\n",
        "- True positives (TP) are failures correctly predicted by the model.\n",
        "- False negatives (FN) are real failures in a generator where there is no detection by model.\n",
        "- False positives (FP) are failure detections in a generator where there is no failure.\n",
        "\n",
        "**Which metric to optimize?**\n",
        "\n",
        "* We need to choose the metric which will ensure that the maximum number of generator failures are predicted correctly by the model.\n",
        "* We would want Recall to be maximized as greater the Recall, the higher the chances of minimizing false negatives.\n",
        "* We want to minimize false negatives because if a model predicts that a machine will have no failure when there will be a failure, it will increase the maintenance cost."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djQTqGKU4jap"
      },
      "source": [
        "**Let's define a function to output different metrics (including recall) on the train and test set and a function to show confusion matrix so that we do not have to use the same code repetitively while evaluating models.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bIekBxwp4jaq"
      },
      "outputs": [],
      "source": [
        "# defining a function to compute different metrics to check performance of a classification model built using sklearn\n",
        "def model_performance_classification_sklearn(model, predictors, target):\n",
        "    \"\"\"\n",
        "    Function to compute different metrics to check classification model performance\n",
        "\n",
        "    model: classifier\n",
        "    predictors: independent variables\n",
        "    target: dependent variable\n",
        "    \"\"\"\n",
        "\n",
        "    # predicting using the independent variables\n",
        "    pred = model.predict(predictors)\n",
        "\n",
        "    acc = accuracy_score(target, pred)  # to compute Accuracy\n",
        "    recall = recall_score(target, pred)  # to compute Recall\n",
        "    precision = precision_score(target, pred)  # to compute Precision\n",
        "    f1 = f1_score(target, pred)  # to compute F1-score\n",
        "\n",
        "    # creating a dataframe of metrics\n",
        "    df_perf = pd.DataFrame(\n",
        "        {\n",
        "            \"Accuracy\": acc,\n",
        "            \"Recall\": recall,\n",
        "            \"Precision\": precision,\n",
        "            \"F1\": f1\n",
        "\n",
        "        },\n",
        "        index=[0],\n",
        "    )\n",
        "\n",
        "    return df_perf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def confusion_matrix_sklearn(model, predictors, target):\n",
        "    \"\"\"\n",
        "    To plot the confusion_matrix with percentages\n",
        "\n",
        "    model: classifier\n",
        "    predictors: independent variables\n",
        "    target: dependent variable\n",
        "    \"\"\"\n",
        "    y_pred = model.predict(predictors)\n",
        "    cm = confusion_matrix(target, y_pred)\n",
        "    labels = np.asarray(\n",
        "        [\n",
        "            [\"{0:0.0f}\".format(item) + \"\\n{0:.2%}\".format(item / cm.flatten().sum())]\n",
        "            for item in cm.flatten()\n",
        "        ]\n",
        "    ).reshape(2, 2)\n",
        "\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    sns.heatmap(cm, annot=labels, fmt=\"\")\n",
        "    plt.ylabel(\"True label\")\n",
        "    plt.xlabel(\"Predicted label\")"
      ],
      "metadata": {
        "id": "d12HvQNaIZkR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqCDCbcw4jas"
      },
      "source": [
        "### Model Building with original data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBtuhurlhKyp"
      },
      "source": [
        "Sample code for model building with original data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V-tpzI7g4jas"
      },
      "outputs": [],
      "source": [
        "models = []  # Empty list to store all the models\n",
        "\n",
        "# Appending models into the list\n",
        "models.append((\"Bagging\", BaggingClassifier(random_state=1)))\n",
        "models.append((\"Random forest\", RandomForestClassifier(random_state=1)))\n",
        "models.append(('Decision tree', DecisionTreeClassifier(random_state=1)))\n",
        "models.append(('AdaBoost', AdaBoostClassifier(random_state=1)))\n",
        "models.append(('Gradient Boosting', GradientBoostingClassifier(random_state=1)))\n",
        "models.append(('XGBoost', XGBClassifier(random_state=1)))\n",
        "\n",
        "print(\"\\n\" \"Training Performance:\" \"\\n\")\n",
        "for name, model in models:\n",
        "    model.fit(X_train, y_train)\n",
        "    scores = recall_score(y_train, model.predict(X_train))\n",
        "    print(\"{}: {}\".format(name, scores))\n",
        "\n",
        "print(\"\\n\" \"Validation Performance:\" \"\\n\")\n",
        "\n",
        "for name, model in models:\n",
        "    model.fit(X_train, y_train)\n",
        "    scores_val = recall_score(y_val, model.predict(X_val))\n",
        "    print(\"{}: {}\".format(name, scores_val))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* XGBoost has the best performance on the original data.\n",
        "  * There is overfitting in these models and the class weights are inbalanced"
      ],
      "metadata": {
        "id": "TqDwdAufJz82"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBKJaFU24jas"
      },
      "source": [
        "### Model Building with Oversampled data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FKxnygkE4jat"
      },
      "outputs": [],
      "source": [
        "print(\"Before Oversampling, counts of label 'Yes': {}\".format(sum(y_train == 1)))\n",
        "print(\"Before Oversampling, counts of label 'No': {} \\n\".format(sum(y_train == 0)))\n",
        "\n",
        "sm = SMOTE(\n",
        "    sampling_strategy=1, k_neighbors=5, random_state=1\n",
        ")  # Synthetic Minority Over Sampling Technique\n",
        "X_train_over, y_train_over = sm.fit_resample(X_train, y_train)\n",
        "\n",
        "\n",
        "print(\"After Oversampling, counts of label 'Yes': {}\".format(sum(y_train_over == 1)))\n",
        "print(\"After Oversampling, counts of label 'No': {} \\n\".format(sum(y_train_over == 0)))\n",
        "\n",
        "\n",
        "print(\"After Oversampling, the shape of train_X: {}\".format(X_train_over.shape))\n",
        "print(\"After Oversampling, the shape of train_y: {} \\n\".format(y_train_over.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uYDlbnUO4jat"
      },
      "outputs": [],
      "source": [
        "models = []  # Empty list to store all the models\n",
        "\n",
        "# Appending models into the list\n",
        "models.append((\"Bagging\", BaggingClassifier(random_state=1)))\n",
        "models.append((\"Random forest\", RandomForestClassifier(random_state=1)))\n",
        "models.append(('Decision tree', DecisionTreeClassifier(random_state=1)))\n",
        "models.append(('AdaBoost', AdaBoostClassifier(random_state=1)))\n",
        "models.append(('Gradient Boosting', GradientBoostingClassifier(random_state=1)))\n",
        "models.append(('XGBoost', XGBClassifier(random_state=1)))\n",
        "\n",
        "print(\"\\n\" \"Training Performance:\" \"\\n\")\n",
        "for name, model in models:\n",
        "    model.fit(X_train_over, y_train_over)\n",
        "    scores = recall_score(y_train_over, model.predict(X_train_over))\n",
        "    print(\"{}: {}\".format(name, scores))\n",
        "\n",
        "print(\"\\n\" \"Validation Performance:\" \"\\n\")\n",
        "\n",
        "for name, model in models:\n",
        "    model.fit(X_train_over, y_train_over)\n",
        "    scores = recall_score(y_val, model.predict(X_val))\n",
        "    print(\"{}: {}\".format(name, scores))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nTraining and Validation Performance Difference:\\n\")\n",
        "\n",
        "for name, model in models:\n",
        "    model.fit(X_train_over, y_train_over)\n",
        "    scores_train = recall_score(y_train_over, model.predict(X_train_over))\n",
        "    scores_val = recall_score(y_val, model.predict(X_val))\n",
        "    difference2 = scores_train - scores_val\n",
        "    print(\"{}: Training Score: {:.4f}, Validation Score: {:.4f}, Difference: {:.4f}\".format(name, scores_train, scores_val, difference2))"
      ],
      "metadata": {
        "id": "YooAHVqnKlZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* XGBoost has the best score followed by AdaBoosting"
      ],
      "metadata": {
        "id": "vARM3F4vK3rR"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1aimb6bn4jat"
      },
      "source": [
        "### Model Building with Undersampled data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DhxfTkvu4jat"
      },
      "outputs": [],
      "source": [
        "# Random undersampler for under sampling the data\n",
        "rus = RandomUnderSampler(random_state=1, sampling_strategy=1)\n",
        "X_train_un, y_train_un = rus.fit_resample(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jROP_DVF4jau"
      },
      "outputs": [],
      "source": [
        "print(\"Before Under Sampling, counts of label 'Yes': {}\".format(sum(y_train == 1)))\n",
        "print(\"Before Under Sampling, counts of label 'No': {} \\n\".format(sum(y_train == 0)))\n",
        "\n",
        "print(\"After Under Sampling, counts of label 'Yes': {}\".format(sum(y_train_un == 1)))\n",
        "print(\"After Under Sampling, counts of label 'No': {} \\n\".format(sum(y_train_un == 0)))\n",
        "\n",
        "print(\"After Under Sampling, the shape of train_X: {}\".format(X_train_un.shape))\n",
        "print(\"After Under Sampling, the shape of train_y: {} \\n\".format(y_train_un.shape))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "models = []  # Empty list to store all the models\n",
        "\n",
        "# Appending models into the list\n",
        "models.append((\"Bagging\", BaggingClassifier(random_state=1)))\n",
        "models.append((\"Random forest\", RandomForestClassifier(random_state=1)))\n",
        "models.append(('Decision tree', DecisionTreeClassifier(random_state=1)))\n",
        "models.append(('AdaBoost', AdaBoostClassifier(random_state=1)))\n",
        "models.append(('Gradient Boosting', GradientBoostingClassifier(random_state=1)))\n",
        "models.append(('XGBoost', XGBClassifier(random_state=1)))\n",
        "\n",
        "\n",
        "print(\"\\n\" \"Training Performance:\" \"\\n\")\n",
        "for name, model in models:\n",
        "    model.fit(X_train_un, y_train_un)\n",
        "    scores = recall_score(y_train_un, model.predict(X_train_un))\n",
        "    print(\"{}: {}\".format(name, scores))\n",
        "\n",
        "print(\"\\n\" \"Validation Performance:\" \"\\n\")\n",
        "\n",
        "for name, model in models:\n",
        "    model.fit(X_train_un, y_train_un)\n",
        "    scores = recall_score(y_val, model.predict(X_val))\n",
        "    print(\"{}: {}\".format(name, scores))"
      ],
      "metadata": {
        "id": "479yCDCaLCe7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nTraining and Validation Performance Difference:\\n\")\n",
        "\n",
        "for name, model in models:\n",
        "    model.fit(X_train_un, y_train_un)\n",
        "    scores_train = recall_score(y_train_un, model.predict(X_train_un))\n",
        "    scores_val = recall_score(y_val, model.predict(X_val))\n",
        "    difference3 = scores_train - scores_val\n",
        "    print(\"{}: Training Score: {:.4f}, Validation Score: {:.4f}, Difference: {:.4f}\".format(name, scores_train, scores_val, difference3))"
      ],
      "metadata": {
        "id": "X5OQhA7rLHiI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* XGBoost has the best score with AdaBoost and Gradient Boosting close behind\n",
        "  * With undersampling, AdaBoost performed better on the Validation set than on the Training set\n",
        "\n",
        "* After building 18 models, it was observed that XGBoost, AdaBoost, and Gradient Boost performed best on an undersampled dataset. XGBoost and AdaBoost performed the best on the oversampled dataset."
      ],
      "metadata": {
        "id": "BfR4aUTbLTfO"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZGY1eL84jau"
      },
      "source": [
        "### HyperparameterTuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxM3jQuK_Pqc"
      },
      "source": [
        "#### Sample Parameter Grids"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note**\n",
        "\n",
        "1. Sample parameter grids have been provided to do necessary hyperparameter tuning. These sample grids are expected to provide a balance between model performance improvement and execution time. One can extend/reduce the parameter grid based on execution time and system configuration.\n",
        "  - Please note that if the parameter grid is extended to improve the model performance further, the execution time will increase\n"
      ],
      "metadata": {
        "id": "MSSmdBoHDJwO"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czq7BZ5b4jau"
      },
      "source": [
        "- For Gradient Boosting:\n",
        "\n",
        "```\n",
        "param_grid = {\n",
        "    \"init\": [AdaBoostClassifier(random_state=1),DecisionTreeClassifier(random_state=1)],\n",
        "    \"n_estimators\": np.arange(50,110,25),\n",
        "    \"learning_rate\": [0.01,0.1,0.05],\n",
        "    \"subsample\":[0.7,0.9],\n",
        "    \"max_features\":[0.5,0.7,1],\n",
        "}\n",
        "```\n",
        "\n",
        "- For Adaboost:\n",
        "\n",
        "```\n",
        "param_grid = {\n",
        "    \"n_estimators\": np.arange(50,110,25),\n",
        "    \"learning_rate\": [0.01,0.1,0.05],\n",
        "    \"base_estimator\": [\n",
        "        DecisionTreeClassifier(max_depth=2, random_state=1),\n",
        "        DecisionTreeClassifier(max_depth=3, random_state=1),\n",
        "    ],\n",
        "}\n",
        "```\n",
        "\n",
        "- For Bagging Classifier:\n",
        "\n",
        "```\n",
        "param_grid = {\n",
        "    'max_samples': [0.8,0.9,1],\n",
        "    'max_features': [0.7,0.8,0.9],\n",
        "    'n_estimators' : [30,50,70],\n",
        "}\n",
        "```\n",
        "- For Random Forest:\n",
        "\n",
        "```\n",
        "param_grid = {\n",
        "    \"n_estimators\": [50,110,25],\n",
        "    \"min_samples_leaf\": np.arange(1, 4),\n",
        "    \"max_features\": [np.arange(0.3, 0.6, 0.1),'sqrt'],\n",
        "    \"max_samples\": np.arange(0.4, 0.7, 0.1)\n",
        "}\n",
        "```\n",
        "\n",
        "- For Decision Trees:\n",
        "\n",
        "```\n",
        "param_grid = {\n",
        "    'max_depth': np.arange(2,6),\n",
        "    'min_samples_leaf': [1, 4, 7],\n",
        "    'max_leaf_nodes' : [10, 15],\n",
        "    'min_impurity_decrease': [0.0001,0.001]\n",
        "}\n",
        "```\n",
        "\n",
        "- For XGBoost (optional):\n",
        "\n",
        "```\n",
        "param_grid={'n_estimators':np.arange(50,110,25),\n",
        "            'scale_pos_weight':[1,2,5],\n",
        "            'learning_rate':[0.01,0.1,0.05],\n",
        "            'gamma':[1,3],\n",
        "            'subsample':[0.7,0.9]\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decision Tree Classifier"
      ],
      "metadata": {
        "id": "r-RFkNltOLEh"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMReRXdH_YUd"
      },
      "source": [
        "#### Sample tuning method for Decision tree with original data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o9kks1hG_Xhy"
      },
      "outputs": [],
      "source": [
        "# defining model\n",
        "Model = DecisionTreeClassifier(random_state=1)\n",
        "\n",
        "# Parameter grid to pass in RandomSearchCV\n",
        "param_grid = {'max_depth': np.arange(2,6),\n",
        "              'min_samples_leaf': [1, 4, 7],\n",
        "              'max_leaf_nodes' : [10,15],\n",
        "              'min_impurity_decrease': [0.0001,0.001] }\n",
        "\n",
        "# Type of scoring used to compare parameter combinations\n",
        "scorer = metrics.make_scorer(metrics.recall_score)\n",
        "\n",
        "#Calling RandomizedSearchCV\n",
        "randomized_cv = RandomizedSearchCV(estimator=Model, param_distributions=param_grid, n_iter=10, n_jobs = -1, scoring=scorer, cv=5, random_state=1)\n",
        "\n",
        "#Fitting parameters in RandomizedSearchCV\n",
        "randomized_cv.fit(X_train,y_train)\n",
        "\n",
        "print(\"Best parameters are {} with CV score={}:\" .format(randomized_cv.best_params_,randomized_cv.best_score_))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chN8hbfThKyr"
      },
      "source": [
        "#### Sample tuning method for Decision tree with oversampled data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tVZcJ0hv4jau"
      },
      "outputs": [],
      "source": [
        "# defining model\n",
        "Model = DecisionTreeClassifier(random_state=1)\n",
        "\n",
        "# Parameter grid to pass in RandomSearchCV\n",
        "param_grid = {'max_depth': np.arange(2,6),\n",
        "              'min_samples_leaf': [1, 4, 7],\n",
        "              'max_leaf_nodes' : [10,15],\n",
        "              'min_impurity_decrease': [0.0001,0.001] }\n",
        "\n",
        "# Type of scoring used to compare parameter combinations\n",
        "scorer = metrics.make_scorer(metrics.recall_score)\n",
        "\n",
        "#Calling RandomizedSearchCV\n",
        "randomized_cv = RandomizedSearchCV(estimator=Model, param_distributions=param_grid, n_iter=10, n_jobs = -1, scoring=scorer, cv=5, random_state=1)\n",
        "\n",
        "#Fitting parameters in RandomizedSearchCV\n",
        "randomized_cv.fit(X_train_over,y_train_over)\n",
        "\n",
        "print(\"Best parameters are {} with CV score={}:\" .format(randomized_cv.best_params_,randomized_cv.best_score_))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtPIiIS7hKyr"
      },
      "source": [
        "#### Sample tuning method for Decision tree with undersampled data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5pbdykhHhKyr"
      },
      "outputs": [],
      "source": [
        "# defining model\n",
        "Model = DecisionTreeClassifier(random_state=1)\n",
        "\n",
        "# Parameter grid to pass in RandomSearchCV\n",
        "param_grid = {'max_depth': np.arange(2,6),\n",
        "              'min_samples_leaf': [1, 4, 7],\n",
        "              'max_leaf_nodes' : [10,15],\n",
        "              'min_impurity_decrease': [0.0001,0.001] }\n",
        "\n",
        "# Type of scoring used to compare parameter combinations\n",
        "scorer = metrics.make_scorer(metrics.recall_score)\n",
        "\n",
        "#Calling RandomizedSearchCV\n",
        "randomized_cv = RandomizedSearchCV(estimator=Model, param_distributions=param_grid, n_iter=10, n_jobs = -1, scoring=scorer, cv=5, random_state=1)\n",
        "\n",
        "#Fitting parameters in RandomizedSearchCV\n",
        "randomized_cv.fit(X_train_un,y_train_un)\n",
        "\n",
        "print(\"Best parameters are {} with CV score={}:\" .format(randomized_cv.best_params_,randomized_cv.best_score_))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tuned_dtc = DecisionTreeClassifier(\n",
        "    min_samples_leaf = 7,\n",
        "    min_impurity_decrease = 0.0001,\n",
        "    max_leaf_nodes = 15,\n",
        "    max_depth = 5,\n",
        "    random_state=1\n",
        ")\n",
        "tuned_dtc.fit(X_train_un, y_train_un)"
      ],
      "metadata": {
        "id": "pLDhJj5HNY2Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking model's performance on training set\n",
        "dtc_train = model_performance_classification_sklearn(tuned_dtc, X_train_un, y_train_un)\n",
        "dtc_train"
      ],
      "metadata": {
        "id": "o2BG_sx2N8-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking model's performance on validation set\n",
        "dtc_val = model_performance_classification_sklearn(tuned_dtc, X_val, y_val)\n",
        "dtc_val"
      ],
      "metadata": {
        "id": "HvTLJA0sN-ZT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### AdaBoost Classifier\n"
      ],
      "metadata": {
        "id": "8DTg2xnUVsZ4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####  AdaBoost on Original Data"
      ],
      "metadata": {
        "id": "sQT6wHKNTTlu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# defining model\n",
        "Model = AdaBoostClassifier(random_state=1)\n",
        "\n",
        "# Parameter grid to pass in RandomSearchCV\n",
        "param_grid = {\n",
        "    \"n_estimators\": np.arange(50,110,25),\n",
        "    \"learning_rate\": [0.01,0.1,0.05],\n",
        "    \"base_estimator\": [\n",
        "        DecisionTreeClassifier(max_depth=2, random_state=1),\n",
        "        DecisionTreeClassifier(max_depth=3, random_state=1),\n",
        "    ],\n",
        "}\n",
        "\n",
        "# Type of scoring used to compare parameter combinations\n",
        "scorer = metrics.make_scorer(metrics.recall_score)\n",
        "\n",
        "#Calling RandomizedSearchCV\n",
        "randomized_cv = RandomizedSearchCV(estimator=Model, param_distributions=param_grid, n_iter=10, n_jobs = -1, scoring=scorer, cv=5, random_state=1)\n",
        "\n",
        "#Fitting parameters in RandomizedSearchCV\n",
        "randomized_cv.fit(X_train,y_train)\n",
        "\n",
        "print(\"Best parameters are {} with CV score={}:\" .format(randomized_cv.best_params_,randomized_cv.best_score_))"
      ],
      "metadata": {
        "id": "9DZGBX2YTX1z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tuned_ada = AdaBoostClassifier(\n",
        "    n_estimators = 100,\n",
        "    learning_rate = 0.1,\n",
        "    base_estimator =\n",
        "      DecisionTreeClassifier(max_depth=3, random_state=1)\n",
        ")\n",
        "tuned_ada.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "Ukvui0FWTrQ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking model's performance on training set\n",
        "ada_train = model_performance_classification_sklearn(tuned_ada, X_train, y_train)\n",
        "ada_train"
      ],
      "metadata": {
        "id": "eY7rUz8iTsnH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking model's performance on validation set\n",
        "ada_val = model_performance_classification_sklearn(tuned_ada, X_val, y_val)\n",
        "ada_val"
      ],
      "metadata": {
        "id": "xJK2Gh1RTtup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### AdaBoost on Undersampled Data"
      ],
      "metadata": {
        "id": "QqjJV6GqWHNb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# defining model\n",
        "Model = AdaBoostClassifier(random_state=1)\n",
        "\n",
        "# Parameter grid to pass in RandomSearchCV\n",
        "param_grid = {\n",
        "    \"n_estimators\": np.arange(50,110,25),\n",
        "    \"learning_rate\": [0.01,0.1,0.05],\n",
        "    \"base_estimator\": [\n",
        "        DecisionTreeClassifier(max_depth=2, random_state=1),\n",
        "        DecisionTreeClassifier(max_depth=3, random_state=1),\n",
        "    ],\n",
        "}\n",
        "\n",
        "# Type of scoring used to compare parameter combinations\n",
        "scorer = metrics.make_scorer(metrics.recall_score)\n",
        "\n",
        "#Calling RandomizedSearchCV\n",
        "randomized_cv = RandomizedSearchCV(estimator=Model, param_distributions=param_grid, n_iter=10, n_jobs = -1, scoring=scorer, cv=5, random_state=1)\n",
        "\n",
        "#Fitting parameters in RandomizedSearchCV\n",
        "randomized_cv.fit(X_train_un,y_train_un)\n",
        "\n",
        "print(\"Best parameters are {} with CV score={}:\" .format(randomized_cv.best_params_,randomized_cv.best_score_))"
      ],
      "metadata": {
        "id": "FxDxdw-AT-tQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tuned_ada_un = AdaBoostClassifier(\n",
        "    n_estimators = 100,\n",
        "    learning_rate = 0.1,\n",
        "    base_estimator =\n",
        "      DecisionTreeClassifier(max_depth=3, random_state=1)\n",
        ")\n",
        "tuned_ada_un.fit(X_train_un, y_train_un)"
      ],
      "metadata": {
        "id": "fn105Sq_WPmz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking model's performance on training set\n",
        "ada_train_un = model_performance_classification_sklearn(tuned_ada_un, X_train_un, y_train_un)\n",
        "ada_train_un"
      ],
      "metadata": {
        "id": "5K0L19sDWQwX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking model's performance on validation set\n",
        "ada_val_un = model_performance_classification_sklearn(tuned_ada_un, X_val, y_val)\n",
        "ada_val_un"
      ],
      "metadata": {
        "id": "9p6Zq0A7WR6N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### AdaBoost on Oversampled Data"
      ],
      "metadata": {
        "id": "jMyeSpIGXQeN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# defining model\n",
        "Model = AdaBoostClassifier(random_state=1)\n",
        "\n",
        "# Parameter grid to pass in RandomSearchCV\n",
        "param_grid = {\n",
        "    \"n_estimators\": np.arange(50,110,25),\n",
        "    \"learning_rate\": [0.01,0.1,0.05],\n",
        "    \"base_estimator\": [\n",
        "        DecisionTreeClassifier(max_depth=2, random_state=1),\n",
        "        DecisionTreeClassifier(max_depth=3, random_state=1),\n",
        "    ],\n",
        "}\n",
        "\n",
        "# Type of scoring used to compare parameter combinations\n",
        "scorer = metrics.make_scorer(metrics.recall_score)\n",
        "\n",
        "#Calling RandomizedSearchCV\n",
        "randomized_cv = RandomizedSearchCV(estimator=Model, param_distributions=param_grid, n_iter=10, n_jobs = -1, scoring=scorer, cv=5, random_state=1)\n",
        "\n",
        "#Fitting parameters in RandomizedSearchCV\n",
        "randomized_cv.fit(X_train_over,y_train_over)\n",
        "\n",
        "print(\"Best parameters are {} with CV score={}:\" .format(randomized_cv.best_params_,randomized_cv.best_score_))"
      ],
      "metadata": {
        "id": "kMyU0UnxXTIT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tuned_ada_over = AdaBoostClassifier(\n",
        "    n_estimators = 100,\n",
        "    learning_rate = 0.1,\n",
        "    base_estimator =\n",
        "      DecisionTreeClassifier(max_depth=3, random_state=1)\n",
        ")\n",
        "tuned_ada_over.fit(X_train_over, y_train_over)"
      ],
      "metadata": {
        "id": "QeyVeBtaXYyU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking model's performance on training set\n",
        "ada_train_over = model_performance_classification_sklearn(tuned_ada_over, X_train_over, y_train_over)\n",
        "ada_train_over"
      ],
      "metadata": {
        "id": "44Yhnm4pXZ4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking model's performance on validation set\n",
        "ada_val_over = model_performance_classification_sklearn(tuned_ada_over, X_val, y_val)\n",
        "ada_val_over"
      ],
      "metadata": {
        "id": "FScPbR5nXa88"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Of all of the AdaBoost models after hyperparameter tuning, the model built with the undersampled dataset performed the best"
      ],
      "metadata": {
        "id": "vMuvbwhPYIwT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gradient Boosting Classifier"
      ],
      "metadata": {
        "id": "PHTqw-YQYvOY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Gradient Boost on Undersampled Data"
      ],
      "metadata": {
        "id": "yaUvZI_CXrhj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# defining model\n",
        "Model = GradientBoostingClassifier(random_state=1)\n",
        "\n",
        "# Parameter grid to pass in RandomSearchCV\n",
        "param_grid = {\n",
        "    \"init\": [AdaBoostClassifier(random_state=1),DecisionTreeClassifier(random_state=1)],\n",
        "    \"n_estimators\": np.arange(50,110,25),\n",
        "    \"learning_rate\": [0.01,0.1,0.05],\n",
        "    \"subsample\":[0.7,0.9],\n",
        "    \"max_features\":[0.5,0.7,1],\n",
        "}\n",
        "\n",
        "# Type of scoring used to compare parameter combinations\n",
        "scorer = metrics.make_scorer(metrics.recall_score)\n",
        "\n",
        "#Calling RandomizedSearchCV\n",
        "randomized_cv = RandomizedSearchCV(estimator=Model, param_distributions=param_grid, n_iter=10, n_jobs = -1, scoring=scorer, cv=5, random_state=1)\n",
        "\n",
        "#Fitting parameters in RandomizedSearchCV\n",
        "randomized_cv.fit(X_train_un,y_train_un)\n",
        "\n",
        "print(\"Best parameters are {} with CV score={}:\" .format(randomized_cv.best_params_,randomized_cv.best_score_))"
      ],
      "metadata": {
        "id": "vXtbchKiXuof"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tuned_gbc_un = GradientBoostingClassifier(\n",
        "  subsample = 0.7,\n",
        "  n_estimators = 100,\n",
        "  max_features = 0.7,\n",
        "  learning_rate = 0.05,\n",
        "  init = DecisionTreeClassifier(random_state=1)\n",
        ")\n",
        "tuned_gbc_un.fit(X_train_un, y_train_un)"
      ],
      "metadata": {
        "id": "QBnVUEa4Y80T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking model's performance on training set\n",
        "gbc_train_un = model_performance_classification_sklearn(tuned_gbc_un, X_train_un, y_train_un)\n",
        "gbc_train_un"
      ],
      "metadata": {
        "id": "g0bD1XNwY-yd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking model's performance on validation set\n",
        "gbc_val_un = model_performance_classification_sklearn(tuned_gbc_un, X_val, y_val)\n",
        "gbc_val_un"
      ],
      "metadata": {
        "id": "Nc31WSm3ZCFO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### XGBoost Classifier"
      ],
      "metadata": {
        "id": "JFjtjE0DZvc6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### XGBoost Classifier on Original Data"
      ],
      "metadata": {
        "id": "ZFryYc9BZ0Uw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# defining model\n",
        "Model = XGBClassifier(random_state=1)\n",
        "\n",
        "# Parameter grid to pass in RandomSearchCV\n",
        "param_grid={'n_estimators':np.arange(50,110,25),\n",
        "            'scale_pos_weight':[1,2,5],\n",
        "            'learning_rate':[0.01,0.1,0.05],\n",
        "            'gamma':[1,3],\n",
        "            'subsample':[0.7,0.9]\n",
        "}\n",
        "\n",
        "# Type of scoring used to compare parameter combinations\n",
        "scorer = metrics.make_scorer(metrics.recall_score)\n",
        "\n",
        "#Calling RandomizedSearchCV\n",
        "randomized_cv = RandomizedSearchCV(estimator=Model, param_distributions=param_grid, n_iter=10, n_jobs = -1, scoring=scorer, cv=5, random_state=1)\n",
        "\n",
        "#Fitting parameters in RandomizedSearchCV\n",
        "randomized_cv.fit(X_train,y_train)\n",
        "\n",
        "print(\"Best parameters are {} with CV score={}:\" .format(randomized_cv.best_params_,randomized_cv.best_score_))"
      ],
      "metadata": {
        "id": "-oBjGzm0Z5k0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tuned_xgb = XGBClassifier(\n",
        "    subsample = 0.9,\n",
        "    scale_pos_weight = 5,\n",
        "    n_estimators = 100,\n",
        "    learning_rate = 0.1,\n",
        "    gamma = 3\n",
        ")\n",
        "tuned_xgb.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "oMF_Zfh1aKmR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking model's performance on training set\n",
        "xgb_train = model_performance_classification_sklearn(tuned_xgb, X_train, y_train)\n",
        "xgb_train"
      ],
      "metadata": {
        "id": "oAEjMMVBaLtQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking model's performance on validation set\n",
        "xgb_val = model_performance_classification_sklearn(tuned_xgb, X_val, y_val)\n",
        "xgb_val"
      ],
      "metadata": {
        "id": "TYWVykIYaNss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### XGB Classifier on Undersampled Data"
      ],
      "metadata": {
        "id": "DvhPqfR8agsf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# defining model\n",
        "Model = XGBClassifier(random_state=1)\n",
        "\n",
        "# Parameter grid to pass in RandomSearchCV\n",
        "param_grid={'n_estimators':np.arange(50,110,25),\n",
        "            'scale_pos_weight':[1,2,5],\n",
        "            'learning_rate':[0.01,0.1,0.05],\n",
        "            'gamma':[1,3],\n",
        "            'subsample':[0.7,0.9]\n",
        "}\n",
        "\n",
        "# Type of scoring used to compare parameter combinations\n",
        "scorer = metrics.make_scorer(metrics.recall_score)\n",
        "\n",
        "#Calling RandomizedSearchCV\n",
        "randomized_cv = RandomizedSearchCV(estimator=Model, param_distributions=param_grid, n_iter=10, n_jobs = -1, scoring=scorer, cv=5, random_state=1)\n",
        "\n",
        "#Fitting parameters in RandomizedSearchCV\n",
        "randomized_cv.fit(X_train_un,y_train_un)\n",
        "\n",
        "print(\"Best parameters are {} with CV score={}:\" .format(randomized_cv.best_params_,randomized_cv.best_score_))"
      ],
      "metadata": {
        "id": "54zFteUDakOJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tuned_xgb_un = XGBClassifier(\n",
        "    subsample = 0.9,\n",
        "    scale_pos_weight = 5,\n",
        "    n_estimators = 100,\n",
        "    learning_rate = 0.1,\n",
        "    gamma = 3\n",
        ")\n",
        "tuned_xgb_un.fit(X_train_un, y_train_un)"
      ],
      "metadata": {
        "id": "d0bgGP1ban5Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking model's performance on training set\n",
        "xgb_train_un = model_performance_classification_sklearn(tuned_xgb_un, X_train_un, y_train_un)\n",
        "xgb_train_un"
      ],
      "metadata": {
        "id": "TJnwBs9xatnU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking model's performance on validation set\n",
        "xgb_val_un = model_performance_classification_sklearn(tuned_xgb_un, X_val, y_val)\n",
        "xgb_val_un"
      ],
      "metadata": {
        "id": "HQPA6yFOauuH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9JNnpxa4jau"
      },
      "source": [
        "## Model Comparison and Final Model Selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0JG85rkY4jav"
      },
      "outputs": [],
      "source": [
        "# training performance comparison\n",
        "\n",
        "models_train_comp_df = pd.concat(\n",
        "    [\n",
        "        ada_train.T,\n",
        "        ada_train_un.T,\n",
        "        ada_train_over.T,\n",
        "        gbc_train_un.T,\n",
        "        xgb_train.T,\n",
        "        xgb_train_un.T\n",
        "    ],\n",
        "    axis=1,\n",
        ")\n",
        "models_train_comp_df.columns = [\n",
        "    'AdaBoost trained with Original data',\n",
        "    'AdaBoost trained with Undersampled data',\n",
        "    'AdaBoost trained with Oversampled data',\n",
        "    'Gradient boosting trained with Undersampled data',\n",
        "    'XGBoost trained with Original data',\n",
        "    'XGBoost trained with Undersampled data'\n",
        "]\n",
        "print(\"Training performance comparison:\")\n",
        "models_train_comp_df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Validation performance comparison\n",
        "\n",
        "models_train_comp_df = pd.concat(\n",
        "    [\n",
        "        ada_val.T,\n",
        "        ada_val_un.T,\n",
        "        ada_val_over.T,\n",
        "        gbc_val_un.T,\n",
        "        xgb_val.T,\n",
        "        xgb_val_un.T\n",
        "    ],\n",
        "    axis=1,\n",
        ")\n",
        "models_train_comp_df.columns = [\n",
        "    'AdaBoost trained with Original data',\n",
        "    'AdaBoost trained with Undersampled data',\n",
        "    'AdaBoost trained with Oversampled data',\n",
        "    'Gradient boosting trained with Undersampled data',\n",
        "    'XGBoost trained with Original data',\n",
        "    'XGBoost trained with Undersampled data'\n",
        "]\n",
        "print(\"Validation performance comparison:\")\n",
        "models_train_comp_df"
      ],
      "metadata": {
        "id": "d4BnpfS0cGuW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The recall score is highest on XGBoost trained with Undersampled Data\n",
        "  * Our test set will be run will this model"
      ],
      "metadata": {
        "id": "UIKPahuhdCjO"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_pDMFAz4jav"
      },
      "source": [
        "### Test set final performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g8-epsXv4jav"
      },
      "outputs": [],
      "source": [
        "xgb_test = model_performance_classification_sklearn(tuned_xgb_un, X_test, y_test)\n",
        "xgb_test"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The XGBoost Classifier trained with Undersampled data has a Recall score of 98.7% on the unseen test data\n",
        "  * This performance aligns with the metrics achieved with this model on the train and validation set\n",
        "  * Our model generalizes extremely well"
      ],
      "metadata": {
        "id": "qFreaNmfdf49"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feature_names = X_train.columns\n",
        "importances = tuned_xgb_un.feature_importances_\n",
        "indices = np.argsort(importances)\n",
        "\n",
        "plt.figure(figsize=(12, 12))\n",
        "plt.title(\"Feature Importances\")\n",
        "plt.barh(range(len(indices)), importances[indices], color=\"violet\", align=\"center\")\n",
        "plt.yticks(range(len(indices)), [feature_names[i] for i in indices])\n",
        "plt.xlabel(\"Relative Importance\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8w-BvQaaehui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* As expected from the EDA, we can see that Total Transaction Count is the most import feature for making predictions\n",
        "  * Features such as Total Transaction Amount, Total Revolving Balance, and Total Relationship Count are the a few of the other important features in this model"
      ],
      "metadata": {
        "id": "XiVgAi92e2Uq"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5hPmHyR4jaw"
      },
      "source": [
        "# Business Insights and Conclusions"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* When determining whether or not a customer is likely to cancel their credit cards, Thera Bank should primarily look into the customer's credit card usage.\n",
        "  * Total Transaction Count is the most important feature in our model - all customers in the data that attrited used their credit cards fewer than 100 times in the past 12 months. Over 50% of customers that attrited had fewer than 50 transactions in the past 12 months.\n",
        "* Similarly to Transaction Count, Transaction Amount also has a big impact on credit card attrition. Removing outliers, all customers that closed their credit card accounts spent less than appoximately $4000 in the past 12 months.\n",
        "* Another feature that has a significant impact on attrition is total product counts. Almost a third of all customers with only 1 or 2 products from Thera Bank closed their credit card accounts.\n",
        "* Recommendations for retaining credit card customers:\n",
        "  * Getting customers to regularly use their credit cards is shown to be the most valuable approach in credit card retention. Providing incentives on transactions or spend amounts could attract customers to using their card more often.\n",
        "  * Brand loyalty is another important factor in customer retention. Customers that use several other products from Thera Bank in addition to their credit cards are more likely to keep their account active. Promoting Thera Bank's other offering such as checking and savings accounts, IRA, CD's, mortgage, loans, etc. can help get customers to build rapport with Thera Bank."
      ],
      "metadata": {
        "id": "4t08l7-GSVpV"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VB3eO21n_sgt"
      },
      "source": [
        "***"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "9EaJ8AGwpM-2",
        "xxhpZv9y-qTw",
        "UvpMDcaaMKtI",
        "j-yGG8LNSSMa",
        "-YyWJgFlKlWM",
        "knk0w9XH4jao",
        "0J99-7Kubp09",
        "YZqmoqz7bp0-",
        "eqCDCbcw4jas",
        "oBKJaFU24jas",
        "1aimb6bn4jat",
        "yZGY1eL84jau",
        "rxM3jQuK_Pqc",
        "GMReRXdH_YUd",
        "chN8hbfThKyr",
        "HtPIiIS7hKyr",
        "D9JNnpxa4jau",
        "d_pDMFAz4jav",
        "c5hPmHyR4jaw"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
